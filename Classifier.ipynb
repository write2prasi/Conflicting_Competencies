{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKAlJluHxhDv"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, confusion_matrix,\n",
        "                            classification_report, make_scorer)\n",
        "from sklearn.pipeline import Pipeline\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, confusion_matrix,\n",
        "                            classification_report, roc_curve, precision_recall_curve)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Time of conflict for different participants (Task 2 - conflict task)\n",
        "toc = {\n",
        "    4: ['15:36:00'], 5: ['18:33:00', '18:44:00'], 6: ['14:57:00'],\n",
        "    7: ['16:02:00'], 8: ['17:04:00'], 12: ['17:01:00'], 13: ['9:49:00'],\n",
        "    14: ['11:18:00'], 16: ['14:14:00'], 17: ['15:42:00'],\n",
        "    18: ['11:22:00', '11:28:00'], 19: ['12:53:00', '13:01:00', '13:02:00'],\n",
        "    20: ['16:24:00', '16:29:00'], 23: ['12:50:00', '12:55:00'],\n",
        "    24: ['14:50:00', '14:53:00'], 25: ['16:18:00', '16:20:00', '16:23:00'],\n",
        "    26: ['10:09:00', '10:16:00'], 27: ['11:14:00', '11:22:00'],\n",
        "    29: ['13:38:00', '13:43:00']\n",
        "}\n",
        "\n",
        "# No-conflict time for Task 1\n",
        "mtt1 = {\n",
        "    1: '10:34:00', 2: '11:52:00', 3: '14:10:00', 4: '15:24:00',\n",
        "    5: '18:21:00', 6: '14:45:00', 7: '15:45:00', 8: '16:51:00',\n",
        "    9: '11:29:00', 10: '5:45:00', 11: '15:00:00', 12: '16:45:00',\n",
        "    13: '9:34:00', 14: '11:04:00', 15: '12:30:00', 16: '13:53:00',\n",
        "    17: '15:09:00', 18: '11:09:00', 19: '12:32:00', 20: '16:16:00',\n",
        "    21: '15:21:00', 23: '12:34:00', 24: '14:34:00', 25: '16:06:00',\n",
        "    26: '9:56:00', 27: '10:56:00', 28: '12:00:00', 29: '13:22:00',\n",
        "    30: '15:06:00'\n",
        "}\n",
        "\n",
        "# Channel mapping\n",
        "CHANNELS = [1, 4, 5, 6, 7, 8]  # FP2, F3, P4, P3, CZ, P4\n",
        "\n",
        "time_format = '%H:%M:%S'\n",
        "\n",
        "def time_to_seconds(time_str):\n",
        "    \"\"\"Convert HH:MM:SS to seconds from midnight\"\"\"\n",
        "    try:\n",
        "        parts = time_str.split(':')\n",
        "        h, m, s = int(parts[0]), int(parts[1]), int(parts[2])\n",
        "        return h * 3600 + m * 60 + s\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def is_conflict(ftime_str, participant_id, window_seconds=60):\n",
        "    \"\"\"\n",
        "    Check if a timestamp is within ±window_seconds of a conflict time\n",
        "\n",
        "    Args:\n",
        "        ftime_str: timestamp string (HH:MM:SS)\n",
        "        participant_id: participant number\n",
        "        window_seconds: window size in seconds (default 60 = 1 minute)\n",
        "\n",
        "    Returns:\n",
        "        1 if conflict, 0 if no conflict\n",
        "    \"\"\"\n",
        "    # Check if participant has conflict data\n",
        "    if participant_id not in toc:\n",
        "        return 0\n",
        "\n",
        "    current_seconds = time_to_seconds(ftime_str)\n",
        "    if current_seconds is None:\n",
        "        return 0\n",
        "\n",
        "    # Check if within window of any conflict time\n",
        "    for conflict_time in toc[participant_id]:\n",
        "        conflict_seconds = time_to_seconds(conflict_time)\n",
        "        if conflict_seconds is None:\n",
        "            continue\n",
        "\n",
        "        if abs(current_seconds - conflict_seconds) <= window_seconds:\n",
        "            return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "def load_and_merge_features():\n",
        "    \"\"\"\n",
        "    Load feature files and merge into single dataset with labels\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with all features and conflict labels\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    # Task 1: No-conflict task\n",
        "    print(\"Loading Task 1 (No Conflict) features...\")\n",
        "    for channel in CHANNELS:\n",
        "        file_path = f'features_1_{channel}.csv'\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['channel'] = channel\n",
        "            df['task'] = 1\n",
        "            all_data.append(df)\n",
        "            print(f\"  Loaded {len(df)} samples from {file_path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  Warning: {file_path} not found\")\n",
        "\n",
        "    # Task 2: Conflict task\n",
        "    print(\"\\nLoading Task 2 (Conflict) features...\")\n",
        "    for channel in CHANNELS:\n",
        "        file_path = f'features_2_{channel}.csv'\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['channel'] = channel\n",
        "            df['task'] = 2\n",
        "            all_data.append(df)\n",
        "            print(f\"  Loaded {len(df)} samples from {file_path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  Warning: {file_path} not found\")\n",
        "\n",
        "    # Combine all data\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    # Add conflict labels based on timestamp\n",
        "    print(\"\\nLabeling conflict instances...\")\n",
        "    combined_df['conflict'] = combined_df.apply(\n",
        "        lambda row: is_conflict(row['ftime'], row['participant_id']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def prepare_features_and_labels(df):\n",
        "    \"\"\"\n",
        "    Prepare feature matrix and labels for classification\n",
        "\n",
        "    Returns:\n",
        "        X: feature matrix\n",
        "        y: labels\n",
        "        df: original dataframe with labels\n",
        "    \"\"\"\n",
        "    # Feature columns (statistical features)\n",
        "    feature_cols = [\n",
        "        'power_1', 'differential_entropy1', 'mean1', 'std1',\n",
        "        'skew1', 'kurtosis1', 'iqr1', 'median1',\n",
        "        'hjorth_01', 'hjorth_11'\n",
        "    ]\n",
        "\n",
        "    # Ensure all feature columns exist\n",
        "    available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "    X = df[available_features].values\n",
        "    y = df['conflict'].values\n",
        "\n",
        "    return X, y, df\n",
        "\n",
        "def train_knn_classifier_5fold(X, y):\n",
        "    \"\"\"\n",
        "    Train k-NN classifier with 5-fold cross-validation\n",
        "\n",
        "    Args:\n",
        "        X: feature matrix\n",
        "        y: labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with cross-validation results\n",
        "    \"\"\"\n",
        "    # Create pipeline with scaling and k-NN\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "    ])\n",
        "\n",
        "    # Define scoring metrics\n",
        "    scoring = {\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'precision': make_scorer(precision_score, zero_division=0),\n",
        "        'recall': make_scorer(recall_score, zero_division=0),\n",
        "        'f1': make_scorer(f1_score, zero_division=0),\n",
        "        'roc_auc': make_scorer(roc_auc_score, needs_proba=True)\n",
        "    }\n",
        "\n",
        "    # 5-fold stratified cross-validation\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    # Perform cross-validation\n",
        "    cv_results = cross_validate(\n",
        "        pipeline, X, y,\n",
        "        cv=cv,\n",
        "        scoring=scoring,\n",
        "        return_train_score=True,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    return cv_results\n",
        "\n",
        "def print_cv_results(cv_results):\n",
        "\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
        "\n",
        "    print(\"\\nTest Performance:\")\n",
        "    for metric in metrics:\n",
        "        test_scores = cv_results[f'test_{metric}']\n",
        "        mean_score = np.mean(test_scores)\n",
        "        std_score = np.std(test_scores)\n",
        "        print(f\"  {metric.upper():12s}: {mean_score:.3f} ± {std_score:.3f}\")\n",
        "        print(f\"    Per fold: {[f'{s:.3f}' for s in test_scores]}\")\n",
        "\n",
        "    print(\"\\nTrain Performance:\")\n",
        "    for metric in metrics:\n",
        "        train_scores = cv_results[f'train_{metric}']\n",
        "        mean_score = np.mean(train_scores)\n",
        "        std_score = np.std(train_scores)\n",
        "        print(f\"  {metric.upper():12s}: {mean_score:.3f} ± {std_score:.3f}\")\n",
        "\n",
        "def train_final_model_and_evaluate(X, y):\n",
        "    \"\"\"\n",
        "    Train final model on all data and show confusion matrix\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Split data for final evaluation\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(classification_report(y_test, y_pred,\n",
        "                               target_names=['No Conflict', 'Conflict']))\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Load and merge features\n",
        "    print(\"\\n1. Loading and merging features from all channels...\")\n",
        "    df = load_and_merge_features()\n",
        "\n",
        "    print(f\"\\nTotal samples: {len(df)}\")\n",
        "    print(f\"Participants: {df['participant_id'].nunique()}\")\n",
        "    print(f\"Tasks: {sorted(df['task'].unique())}\")\n",
        "    print(f\"Channels: {sorted(df['channel'].unique())}\")\n",
        "\n",
        "    # Prepare features and labels\n",
        "    print(\"\\n2. Preparing features and labels...\")\n",
        "    X, y, df_labeled = prepare_features_and_labels(df)\n",
        "\n",
        "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "    print(f\"Class distribution:\")\n",
        "    print(f\"  No Conflict (0): {np.sum(y == 0)} samples ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
        "    print(f\"  Conflict (1):    {np.sum(y == 1)} samples ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
        "\n",
        "    # Train and evaluate with 5-fold CV\n",
        "    print(\"\\n3. Training k-NN classifier with 5-fold cross-validation...\")\n",
        "    cv_results = train_knn_classifier_5fold(X, y)\n",
        "\n",
        "    # Print results\n",
        "    print_cv_results(cv_results)\n",
        "\n",
        "    # Train final model and show confusion matrix\n",
        "    print(\"\\n4. Training final model for visualization...\")\n",
        "    train_final_model_and_evaluate(X, y)\n",
        "\n",
        "    # Save results\n",
        "    results_dict = {\n",
        "        'metric': ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc'],\n",
        "        'mean': [],\n",
        "        'std': []\n",
        "    }\n",
        "\n",
        "    for metric in results_dict['metric']:\n",
        "        test_scores = cv_results[f'test_{metric}']\n",
        "        results_dict['mean'].append(np.mean(test_scores))\n",
        "        results_dict['std'].append(np.std(test_scores))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert time string to datetime object\n",
        "def time_to_datetime(time_str):\n",
        "    \"\"\"Convert time string to datetime object (using arbitrary date)\"\"\"\n",
        "    return datetime.strptime(time_str, '%H:%M:%S')\n",
        "\n",
        "\n",
        "def parse_datetime_to_seconds(datetime_str):\n",
        "    \"\"\"Parse datetime string and extract time in seconds from midnight\"\"\"\n",
        "    try:\n",
        "        datetime_str = str(datetime_str).strip()\n",
        "        if ' ' in datetime_str:\n",
        "            time_part = datetime_str.split(' ')[1]\n",
        "        else:\n",
        "            time_part = datetime_str\n",
        "        parts = time_part.split(':')\n",
        "        h, m, s = int(parts[0]), int(parts[1]), int(parts[2])\n",
        "        return h * 3600 + m * 60 + s\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def is_within_window(datetime_str, target_seconds, window_seconds=60):\n",
        "    \"\"\"Check if datetime/timestamp is within window of target time\"\"\"\n",
        "    try:\n",
        "        ts_seconds = parse_datetime_to_seconds(datetime_str)\n",
        "        if ts_seconds is None:\n",
        "            return False\n",
        "        return (target_seconds - window_seconds) <= ts_seconds <= (target_seconds + window_seconds)\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def extract_features_for_window(participant_id, target_times, task_type, label):\n",
        "    \"\"\"\n",
        "    Extract features from CSV files within ±1 minute window of target times\n",
        "\n",
        "    Args:\n",
        "        participant_id: int, participant number\n",
        "        target_times: list of time strings\n",
        "        task_type: 1 or 2 (for features_1_*.csv or features_2_*.csv)\n",
        "        label: 0 for no conflict, 1 for conflict\n",
        "\n",
        "    Returns: DataFrame with features, label, and participant_id\n",
        "    \"\"\"\n",
        "    feature_file_numbers = [1, 4, 5, 6, 7, 8]\n",
        "    all_samples = []\n",
        "\n",
        "    participant_folder = os.path.join(datadir, str(participant_id))\n",
        "    #print(\"Folder = \"+participant_folder)\n",
        "    if not os.path.exists(participant_folder):\n",
        "        print(f\"  WARNING: Folder not found for participant {participant_id}\")\n",
        "        return None\n",
        "\n",
        "    for target_time in target_times:\n",
        "        target_seconds = time_to_seconds(target_time)\n",
        "\n",
        "        if target_seconds is None:\n",
        "            print(f\"  WARNING: Invalid target time format: {target_time}\")\n",
        "            continue\n",
        "\n",
        "        # Read all 6 files and merge horizontally by timestamp\n",
        "        file_dataframes = []\n",
        "\n",
        "        # # Storage for features from all files for this time window\n",
        "        # window_features = {}\n",
        "        # timestamps_collected = []\n",
        "\n",
        "        for file_num in feature_file_numbers:\n",
        "            file_path = os.path.join(participant_folder, f\"features_{task_type}_{file_num}.csv\")\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "\n",
        "                    # Use 'ftime' as timestamp column\n",
        "                    if 'ftime' not in df.columns:\n",
        "                        print(f\"  ERROR: 'ftime' column not found in {file_path}\")\n",
        "                        continue\n",
        "\n",
        "                    # Filter rows within the time window\n",
        "                    df['time_seconds'] = df['ftime'].apply(parse_datetime_to_seconds)\n",
        "                    matching_rows = df[(df['time_seconds'] >= target_seconds - 60) &\n",
        "                                      (df['time_seconds'] <= target_seconds + 60)]\n",
        "\n",
        "                    if not matching_rows.empty:\n",
        "                        # Keep only feature columns and rename with file number\n",
        "                        feature_cols = [col for col in df.columns if col not in ['ftime', 'time_seconds']]\n",
        "                        matching_rows = matching_rows[['ftime', 'time_seconds'] + feature_cols].copy()\n",
        "\n",
        "                        # Rename feature columns to include file number\n",
        "                        rename_dict = {col: f\"file{file_num}_{col}\" for col in feature_cols}\n",
        "                        matching_rows.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "                        file_dataframes.append(matching_rows)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ERROR reading {file_path}: {e}\")\n",
        "\n",
        "        # Merge all 6 files by timestamp (should have same timestamps)\n",
        "        if len(file_dataframes) == 6:\n",
        "            # Use the first file as base\n",
        "            merged_df = file_dataframes[0].copy()\n",
        "\n",
        "            # Merge other files on 'time_seconds'\n",
        "            for df in file_dataframes[1:]:\n",
        "                merged_df = merged_df.merge(df, on=['ftime', 'time_seconds'], how='inner')\n",
        "\n",
        "            # Add metadata columns\n",
        "            merged_df['label'] = label\n",
        "            merged_df['participant_id'] = participant_id\n",
        "            merged_df['target_time'] = target_time\n",
        "\n",
        "            # Sort by time_seconds to ensure chronological order\n",
        "            merged_df = merged_df.sort_values('time_seconds')\n",
        "\n",
        "            all_samples.append(merged_df)\n",
        "        else:\n",
        "            if len(file_dataframes) > 0:\n",
        "                print(f\"  WARNING: Incomplete data for participant {participant_id}, \"\n",
        "                      f\"time {target_time} (only {len(file_dataframes)}/6 files had matching data)\")\n",
        "\n",
        "    if all_samples:\n",
        "        return pd.concat(all_samples, ignore_index=True)\n",
        "    return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_9NhukLKGbv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and labels\n",
        "X = new_final_dataset.drop(['label', 'participant_id'], axis=1)\n",
        "y = new_final_dataset['label']\n",
        "groups = new_final_dataset['participant_id']"
      ],
      "metadata": {
        "id": "aYfK1OtAISnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for infinity values\n",
        "print(f\"\\nChecking for infinity values...\")\n",
        "inf_mask = np.isinf(X.values)\n",
        "n_inf = inf_mask.sum()\n",
        "print(f\"Number of infinity values: {n_inf}\")\n",
        "\n",
        "if n_inf > 0:\n",
        "    # Find columns with infinity\n",
        "    inf_cols = X.columns[np.isinf(X.values).any(axis=0)]\n",
        "    print(f\"Columns with infinity: {list(inf_cols)}\")\n",
        "\n",
        "    # Replace infinity with NaN\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "    print(\"✓ Replaced infinity values with NaN\")\n",
        "\n",
        "# Check for NaN values\n",
        "n_nan = X.isna().sum().sum()\n",
        "print(f\"\\nNumber of NaN values: {n_nan}\")\n",
        "\n",
        "if n_nan > 0:\n",
        "    # Option 1: Fill NaN with column mean\n",
        "    X = X.fillna(X.mean())\n",
        "    print(\"✓ Filled NaN values with column means\")\n",
        "\n",
        "# Check for extremely large values\n",
        "print(f\"\\nChecking for extremely large values...\")\n",
        "max_val = X.max().max()\n",
        "min_val = X.min().min()\n",
        "print(f\"Maximum value in dataset: {max_val}\")\n",
        "print(f\"Minimum value in dataset: {min_val}\")\n",
        "\n",
        "# Clip extreme values (optional - if you have unreasonably large values)\n",
        "if max_val > 1e10 or min_val < -1e10:\n",
        "    print(f\"WARNING: Extremely large values detected!\")\n",
        "    # Clip to reasonable range (adjust these thresholds based on your data)\n",
        "    X = X.clip(lower=-1e10, upper=1e10)\n",
        "    print(\"✓ Clipped extreme values\")"
      ],
      "metadata": {
        "id": "u6C0ii_kJJCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LOPO cross-validator\n",
        "logo = LeaveOneGroupOut()\n",
        "n_splits = logo.get_n_splits(groups=groups)\n",
        "print(f\"\\nNumber of LOPO folds: {n_splits}\")\n",
        "\n",
        "\n",
        "# Storage for results\n",
        "results = {\n",
        "    'participant_id': [],\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'f1_score': [],\n",
        "    'auc': [],\n",
        "    'n_samples_test': [],\n",
        "    'n_class0_test': [],\n",
        "    'n_class1_test': []\n",
        "}\n",
        "\n",
        "# Store predictions for overall metrics\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "all_y_proba = []\n",
        "\n",
        "# Initialize model\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOPO CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fold = 0\n",
        "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
        "    fold += 1\n",
        "\n",
        "    # Get train and test data\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    test_participant = groups.iloc[test_idx].iloc[0]\n",
        "\n",
        "    # Standardize features (fit on train, transform both)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    # AUC (handle case where only one class in test set)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "    except:\n",
        "        auc = np.nan\n",
        "\n",
        "    # Store results\n",
        "    results['participant_id'].append(test_participant)\n",
        "    results['accuracy'].append(acc)\n",
        "    results['precision'].append(prec)\n",
        "    results['recall'].append(rec)\n",
        "    results['f1_score'].append(f1)\n",
        "    results['auc'].append(auc)\n",
        "    results['n_samples_test'].append(len(y_test))\n",
        "    results['n_class0_test'].append(sum(y_test == 0))\n",
        "    results['n_class1_test'].append(sum(y_test == 1))\n",
        "\n",
        "    # Aggregate predictions\n",
        "    all_y_true.extend(y_test)\n",
        "    all_y_pred.extend(y_pred)\n",
        "    all_y_proba.extend(y_proba)\n",
        "\n",
        "    # print(f\"Fold {fold:2d} | Participant {test_participant:2d} | \"\n",
        "    #       f\"Samples: {len(y_test):3d} | Acc: {acc:.3f} | \"\n",
        "    #       f\"Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f} | \"\n",
        "    #       f\"AUC: {auc:.3f if not np.isnan(auc) else 'N/A'}\")\n",
        "\n",
        "    print(f\"Fold {fold:2d} | Participant {test_participant:2d} | \"\n",
        "      f\"Samples: {len(y_test):3d} | Acc: {acc:.3f} | \"\n",
        "      f\"Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f} | \"\n",
        "      f\"AUC: {auc:.3f}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PER-PARTICIPANT RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Calculate aggregate statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AGGREGATE STATISTICS (Mean ± Std)\")\n",
        "print(\"=\"*80)\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'auc']:\n",
        "    values = results_df[metric].dropna()\n",
        "    mean = values.mean()\n",
        "    std = values.std()\n",
        "    print(f\"{metric.capitalize():15s}: {mean:.3f} ± {std:.3f}\")\n",
        "\n",
        "# Overall confusion matrix\n",
        "cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL CONFUSION MATRIX\")\n",
        "print(\"=\"*80)\n",
        "print(cm)\n",
        "print(f\"\\nTrue Negatives:  {cm[0,0]}\")\n",
        "print(f\"False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}\")\n",
        "print(f\"True Positives:  {cm[1,1]}\")\n",
        "\n",
        "# Overall metrics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL PERFORMANCE METRICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:  {accuracy_score(all_y_true, all_y_pred):.3f}\")\n",
        "print(f\"Precision: {precision_score(all_y_true, all_y_pred):.3f}\")\n",
        "print(f\"Recall:    {recall_score(all_y_true, all_y_pred):.3f}\")\n",
        "print(f\"F1-Score:  {f1_score(all_y_true, all_y_pred):.3f}\")\n",
        "print(f\"AUC-ROC:   {roc_auc_score(all_y_true, all_y_proba):.3f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(all_y_true, all_y_proba)\n",
        "roc_auc = roc_auc_score(all_y_true, all_y_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - LOPO Cross-Validation')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig('roc_curve_lopo.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ ROC curve saved to: roc_curve_lopo.png\")\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(all_y_true, all_y_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', lw=2)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - LOPO Cross-Validation')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig('pr_curve_lopo.png', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "glgw2SI4JYdc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Dwg6xjvYwM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Channel mapping\n",
        "CHANNELS = {\n",
        "    1: 'FP2',\n",
        "    4: 'F3',\n",
        "    5: 'P4',\n",
        "    6: 'P3',\n",
        "    7: 'CZ',\n",
        "    8: 'F4'\n",
        "}\n",
        "\n",
        "time_format = '%H:%M:%S'  # Adjust if your format is different\n",
        "\n",
        "def statistical_features(data):\n",
        "    \"\"\"\n",
        "    Compute 10 statistical features from raw EEG data\n",
        "\n",
        "    Args:\n",
        "        data: pandas Series or array of raw EEG values\n",
        "\n",
        "    Returns:\n",
        "        numpy array with 10 features\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    # 1. Power (sum of squared values)\n",
        "    features.append(np.sum(data ** 2))\n",
        "\n",
        "    # 2. Differential Entropy (approximation)\n",
        "    # H = 0.5 * log(2*pi*e*variance)\n",
        "    variance = np.var(data)\n",
        "    if variance > 0:\n",
        "        diff_entropy = 0.5 * np.log(2 * np.pi * np.e * variance)\n",
        "    else:\n",
        "        diff_entropy = 0\n",
        "    features.append(diff_entropy)\n",
        "\n",
        "    # 3. Mean\n",
        "    features.append(np.mean(data))\n",
        "\n",
        "    # 4. Standard Deviation\n",
        "    features.append(np.std(data))\n",
        "\n",
        "    # 5. Skewness\n",
        "    from scipy.stats import skew\n",
        "    features.append(skew(data))\n",
        "\n",
        "    # 6. Kurtosis\n",
        "    from scipy.stats import kurtosis\n",
        "    features.append(kurtosis(data))\n",
        "\n",
        "    # 7. IQR (Interquartile Range)\n",
        "    features.append(np.percentile(data, 75) - np.percentile(data, 25))\n",
        "\n",
        "    # 8. Median\n",
        "    features.append(np.median(data))\n",
        "\n",
        "    # 9. Hjorth Activity (variance)\n",
        "    features.append(np.var(data))\n",
        "\n",
        "    # 10. Hjorth Mobility\n",
        "    # Mobility = sqrt(var(diff(signal)) / var(signal))\n",
        "    diff_data = np.diff(data)\n",
        "    if len(diff_data) > 0 and np.var(data) > 0:\n",
        "        mobility = np.sqrt(np.var(diff_data) / np.var(data))\n",
        "    else:\n",
        "        mobility = 0\n",
        "    features.append(mobility)\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def gen_features_all_channels_all_data(task=2, participant_list=None):\n",
        "    \"\"\"\n",
        "    Generate statistical features for ALL data\n",
        "    Processes all 6 channels and saves features separately\n",
        "\n",
        "    Args:\n",
        "        task: task number (1, 2, or 3)\n",
        "        participant_list: list of participant IDs, or None for all\n",
        "    \"\"\"\n",
        "    ten_sec = datetime.timedelta(seconds=10)\n",
        "\n",
        "    # If no participant list provided, use all\n",
        "    if participant_list is None:\n",
        "        participant_list = range(1, 31)  # Participants 1-30\n",
        "\n",
        "    # Process each channel separately\n",
        "    for channel_num, channel_name in CHANNELS.items():\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing Channel {channel_num} ({channel_name}) for Task {task}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        X_channel = None\n",
        "        metadata = []  # Store participant_id and timestamp for each row\n",
        "\n",
        "        for participant in tqdm(participant_list, desc=f\"Task {task} - {channel_name}\"):\n",
        "\n",
        "            # Read raw EEG data for this participant and task\n",
        "            data_file = f'../input/eeg-processed-extended/{participant}/user{participant}_t{task}.csv'\n",
        "\n",
        "            try:\n",
        "                data = pd.read_csv(data_file)\n",
        "\n",
        "                # Clean up columns\n",
        "                if 'Unnamed: 0' in data.columns:\n",
        "                    data = data.drop(columns=['Unnamed: 0'])\n",
        "                if 'val' in data.columns:\n",
        "                    data = data.drop(columns=['val'])\n",
        "\n",
        "                # Parse time column\n",
        "                data['ftime'] = data['time'].map(\n",
        "                    lambda x: datetime.datetime.strptime(\n",
        "                        x[0: x.index('.')].strip() if '.' in x else x.strip(),\n",
        "                        time_format\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # Sort by time\n",
        "                data = data.sort_values('ftime').reset_index(drop=True)\n",
        "\n",
        "                # Get the channel column (adjust column name based on your data)\n",
        "                # Assuming columns are named like '1', '4', '5', etc.\n",
        "                channel_col = str(channel_num)\n",
        "\n",
        "                if channel_col not in data.columns:\n",
        "                    print(f\"  Warning: Channel {channel_col} not found for participant {participant}\")\n",
        "                    continue\n",
        "\n",
        "                # Process data in 10-second windows\n",
        "                i = 0\n",
        "                while i + 1 < len(data):\n",
        "                    current_time = data.iloc[i]['ftime']\n",
        "                    end_time = current_time + ten_sec\n",
        "\n",
        "                    # Get data in this 10-second window\n",
        "                    window_data = data.loc[\n",
        "                        (data['ftime'] >= current_time) &\n",
        "                        (data['ftime'] < end_time)\n",
        "                    ]\n",
        "\n",
        "                    # Only process if we have enough data points\n",
        "                    if len(window_data) >= 5:  # Minimum 5 samples for meaningful stats\n",
        "                        # Extract raw EEG values for this channel\n",
        "                        raw_values = window_data[channel_col].values\n",
        "\n",
        "                        # Compute statistical features\n",
        "                        features = np.expand_dims(statistical_features(raw_values), axis=0)\n",
        "\n",
        "                        # Accumulate features\n",
        "                        X_channel = features if X_channel is None else np.concatenate((X_channel, features))\n",
        "\n",
        "                        # Store metadata\n",
        "                        metadata.append({\n",
        "                            'participant_id': participant,\n",
        "                            'ftime': current_time.strftime(time_format)\n",
        "                        })\n",
        "\n",
        "                    # Move to next window (shift by 10 seconds)\n",
        "                    i += len(window_data)\n",
        "                    if len(window_data) == 0:\n",
        "                        i += 1  # Prevent infinite loop\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                print(f\"  Warning: Data file not found for participant {participant}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing participant {participant}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save features for this channel\n",
        "        if X_channel is not None:\n",
        "            # Save feature matrix\n",
        "            feature_file = f'features_{task}_{channel_num}.csv'\n",
        "\n",
        "            # Create DataFrame with metadata\n",
        "            feature_columns = [\n",
        "                'power_1', 'differential_entropy1', 'mean1', 'std1',\n",
        "                'skew1', 'kurtosis1', 'iqr1', 'median1',\n",
        "                'hjorth_01', 'hjorth_11'\n",
        "            ]\n",
        "\n",
        "            df_features = pd.DataFrame(X_channel, columns=feature_columns)\n",
        "            df_metadata = pd.DataFrame(metadata)\n",
        "\n",
        "            # Combine metadata and features\n",
        "            df_final = pd.concat([df_metadata, df_features], axis=1)\n",
        "\n",
        "            # Save to CSV\n",
        "            df_final.to_csv(feature_file, index=False)\n",
        "\n",
        "            print(f\"\\n✓ Saved {len(df_final)} samples to {feature_file}\")\n",
        "            print(f\"  Shape: {X_channel.shape}\")\n",
        "            print(f\"  Participants: {df_metadata['participant_id'].nunique()}\")\n",
        "        else:\n",
        "            print(f\"\\n✗ No data processed for channel {channel_num}\")\n",
        "\n",
        "\n",
        "# Run feature extraction for all tasks\n",
        "print(\"=\"*80)\n",
        "print(\"STATISTICAL FEATURE EXTRACTION FROM RAW EEG DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extract features for Task 1 (all participants, all data)\n",
        "gen_features_all_channels_all_data(task=1)\n",
        "\n",
        "# Extract features for Task 2 (all participants, all data)\n",
        "gen_features_all_channels_all_data(task=2)\n",
        "\n",
        "# Extract features for Task 3 (all participants, all data)\n",
        "gen_features_all_channels_all_data(task=3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE EXTRACTION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nGenerated files:\")\n",
        "for task in [1, 2, 3]:\n",
        "    for channel_num in CHANNELS.keys():\n",
        "        print(f\"  - features_{task}_{channel_num}.csv\")\n"
      ]
    }
  ]
}